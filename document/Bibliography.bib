@INPROCEEDINGS{autonomous_driving,
  author={Okuyama, Takafumi and Gonsalves, Tad and Upadhay, Jaychand},
  booktitle={2018 International Conference on Intelligent Autonomous Systems (ICoIAS)}, 
  title={Autonomous Driving System based on Deep Q Learnig}, 
  year={2018},
  volume={},
  number={},
  pages={201-205},
  doi={10.1109/ICoIAS.2018.8494053}}

@INPROCEEDINGS{fairness_def,
  author={Verma, Sahil and Rubin, Julia},
  booktitle={2018 IEEE/ACM International Workshop on Software Fairness (FairWare)}, 
  title={Fairness Definitions Explained}, 
  year={2018},
  volume={},
  number={},
  pages={1-7},
  doi={10.1145/3194770.3194776}}

@inproceedings{compas,
  title     = {mdfa: Multi-Differential Fairness Auditor for Black Box Classifiers},
  author    = {Gitiaux, Xavier and Rangwala, Huzefa},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {5871--5879},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/814},
  url       = {https://doi.org/10.24963/ijcai.2019/814},
}

@inproceedings{counterfactual,
 author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Fairness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{fair_awareness,
      title={Fairness Through Awareness}, 
      author={Cynthia Dwork and Moritz Hardt and Toniann Pitassi and Omer Reingold and Rich Zemel},
      year={2011},
      eprint={1104.3913},
      archivePrefix={arXiv},
      primaryClass={cs.CC}
}

@misc{subgroup_fairness,
      title={An Empirical Study of Rich Subgroup Fairness for Machine Learning}, 
      author={Michael Kearns and Seth Neel and Aaron Roth and Zhiwei Steven Wu},
      year={2018},
      eprint={1808.08166},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{resp_data,
author = {Stoyanovich, Julia and Howe, Bill and Jagadish, H. V.},
title = {Responsible Data Management},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415570},
doi = {10.14778/3415478.3415570},
abstract = {The need for responsible data management intensifies with the growing impact of data on society. One central locus of the societal impact of data are Automated Decision Systems (ADS), socio-legal-technical systems that are used broadly in industry, non-profits, and government. ADS process data about people, help make decisions that are consequential to people's lives, are designed with the stated goals of improving efficiency and promoting equitable access to opportunity, involve a combination of human and automated decision making, and are subject to auditing for legal compliance and to public disclosure. They may or may not use AI, and may or may not operate with a high degree of autonomy, but they rely heavily on data.In this article, we argue that the data management community is uniquely positioned to lead the responsible design, development, use, and oversight of ADS. We outline a technical research agenda that requires that we step outside our comfort zone of engineering for efficiency and accuracy, to also incorporate reasoning about values and beliefs. This seems high-risk, but one of the upsides is being able to explain to our children what we do and why it matters.},
journal = {Proc. VLDB Endow.},
month = {8},
pages = {3474--3488},
numpages = {15}
}

@misc{survey,
      title={A Survey on Bias and Fairness in Machine Learning}, 
      author={Ninareh Mehrabi and Fred Morstatter and Nripsuta Saxena and Kristina Lerman and Aram Galstyan},
      year={2022},
      eprint={1908.09635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{compas2,
author = {Julia Dressel  and Hany Farid },
title = {The accuracy, fairness, and limits of predicting recidivism},
journal = {Science Advances},
volume = {4},
number = {1},
pages = {eaao5580},
year = {2018},
doi = {10.1126/sciadv.aao5580},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.aao5580},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.aao5580},
abstract = {Should we trust computers to make life-altering decisions in the criminal justice system? Algorithms for predicting recidivism are commonly used to assess a criminal defendant’s likelihood of committing a crime. These predictions are used in pretrial, parole, and sentencing decisions. Proponents of these systems argue that big data and advanced machine learning make these analyses more accurate and less biased than humans. We show, however, that the widely used commercial risk assessment software COMPAS is no more accurate or fair than predictions made by people with little or no criminal justice expertise. In addition, despite COMPAS’s collection of 137 features, the same accuracy can be achieved with a simple linear classifier with only two features.}}

@misc{understanding,
      title={Understanding racial bias in health using the Medical Expenditure Panel Survey data}, 
      author={Moninder Singh and Karthikeyan Natesan Ramamurthy},
      year={2019},
      eprint={1911.01509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{adult,
      title={Identifying and examining machine learning biases on Adult dataset}, 
      author={Sahil Girhepuje},
      year={2023},
      eprint={2310.09373},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{datta2015automated,
      title={Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice, and Discrimination}, 
      author={Amit Datta and Michael Carl Tschantz and Anupam Datta},
      year={2015},
      eprint={1408.6491},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{sweeney2013discrimination,
      title={Discrimination in Online Ad Delivery}, 
      author={Latanya Sweeney},
      year={2013},
      eprint={1301.6822},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{unequal_rep,
author = {Kay, Matthew and Matuszek, Cynthia and Munson, Sean A.},
title = {Unequal Representation and Gender Stereotypes in Image Search Results for Occupations},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702520},
doi = {10.1145/2702123.2702520},
abstract = {Information environments have the power to affect people's perceptions and behaviors. In this paper, we present the results of studies in which we characterize the gender bias present in image search results for a variety of occupations. We experimentally evaluate the effects of bias in image search results on the images people choose to represent those careers and on people's perceptions of the prevalence of men and women in each occupation. We find evidence for both stereotype exaggeration and systematic underrepresentation of women in search results. We also find that people rate search results higher when they are consistent with stereotypes for a career, and shifting the representation of gender in image search results can shift people's perceptions about real-world distributions. We also discuss tensions between desires for high-quality results and broader societal goals for equality of representation in this space.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {3819–3828},
numpages = {10},
keywords = {inequality, gender, representation, image search, bias, stereotypes},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@techreport{facial_bias,
  doi = {10.5281/ZENODO.4050457},
  url = {https://zenodo.org/record/4050457},
  author = {Leslie, David},
  keywords = {facial recognition technologies, algorithmic bias, digital ethics, responsible innovation, biometric technologies},
  title = {Understanding bias in facial recognition technologies},
  publisher = {Zenodo},
  year = {2020}, 
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{chatbot_bias,
      title={Bias and Fairness in Chatbots: An Overview}, 
      author={Jintang Xue and Yun-Cheng Wang and Chengwei Wei and Xiaofeng Liu and Jonghye Woo and C. -C. Jay Kuo},
      year={2023},
      eprint={2309.08836},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{voice_bias,
author = {Bajorek, Joan},
year = {2019},
month = {05},
pages = {},
title = {Voice Recognition Still Has Significant Race and Gender Biases},
journal = {Harvard business review}
}

@article{preproc,
author = {Kamiran, Faisal and Calders, Toon},
year = {2011},
month = {10},
pages = {},
title = {Data Pre-Processing Techniques for Classification without Discrimination},
volume = {33},
journal = {Knowledge and Information Systems},
doi = {10.1007/s10115-011-0463-8}
}
@article{osti_10182459,
place = {Country unknown/Code not available}, 
title = {Fairness-Aware Instrumentation of Preprocessing~Pipelines for Machine Learning}, 
url = {https://par.nsf.gov/biblio/10182459}, 
DOI = {10.1145/3398730.3399194}, 
abstractNote = {Surfacing and mitigating bias in ML pipelines is a complex topic, with a dire need to provide system-level support to data scientists. Humans should be empowered to debug these pipelines, in order to control for bias and to improve data quality and representativeness. We propose fairDAGs, an open-source library that extracts directed acyclic graph (DAG) representations of the data flow in preprocessing pipelines for ML. The library subsequently instruments the pipelines with tracing and visualization code to capture changes in data distributions and identify distortions with respect to protected group membership as the data travels through the pipeline. We illustrate the utility of fairDAGs, with experiments on publicly available ML pipelines.}, 
journal = {Workshop on Human-In-the-Loop Data Analytics (HILDA'20)}, 
author = {Yang, Ke and Huang, Biao and Stoyanovich, Julia and Schelter, Sebastian}, }

@misc{donini2020empirical,
      title={Empirical Risk Minimization under Fairness Constraints}, 
      author={Michele Donini and Luca Oneto and Shai Ben-David and John Shawe-Taylor and Massimiliano Pontil},
      year={2020},
      eprint={1802.08626},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{zafar17a,
  title = 	 {{Fairness Constraints: Mechanisms for Fair Classification}},
  author = 	 {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P.},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {962--970},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {04},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/zafar17a.html},
  abstract = 	 {Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy.}
}

@inproceedings{ml_in_admissions,
author = {Martinez Neda, Barbara and Zeng, Yue and Gago-Masague, Sergio},
title = {Using Machine Learning in Admissions: Reducing Human and Algorithmic Bias in the Selection Process},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3439664},
doi = {10.1145/3408877.3439664},
abstract = {Diverse classrooms are linked to enhanced intellectual engagement and understanding of different perspectives. College admissions decisions have traditionally relied heavily on academic characteristics like GPA and standardized testing. Universities started to adopt holistic strategies while attempting to increase diversity. Yet, increasing subjective assessment may increase risk of human bias. Machine Learning (ML) could assist in admitting a more diverse student body, but algorithmic bias could be introduced. Our goal is to develop software tools to minimize human bias in admissions while actively eliminating algorithmic bias. We will examine past admissions data and identify risks of use of possible privileged values, which have historically put certain groups at a disadvantage. This tool may help reduce bias risk and select a more diverse and academically prepared group for admission.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {1323},
numpages = {1},
keywords = {diversity, college admission, machine learning},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@article{d_Alessandro_2017,
   title={Conscientious Classification: A Data Scientist’s Guide to Discrimination-Aware Classification},
   volume={5},
   ISSN={2167-647X},
   url={http://dx.doi.org/10.1089/big.2016.0048},
   DOI={10.1089/big.2016.0048},
   number={2},
   journal={Big Data},
   publisher={Mary Ann Liebert Inc},
   author={d’Alessandro, Brian and O’Neil, Cathy and LaGatta, Tom},
   year={2017},
   month=jun, pages={120–134} }

@article{seymour2018bias,
  title = "Detecting Bias: Does an Algorithm Have to Be Transparent in Order to Be Fair?",
  author = "William Seymour",
  year = "2018",
  journal = "BIAS 2018",
  url = "http://ceur-ws.org/Vol-2103/",
}

@misc{agarwal2018reductions,
      title={A Reductions Approach to Fair Classification}, 
      author={Alekh Agarwal and Alina Beygelzimer and Miroslav Dudík and John Langford and Hanna Wallach},
      year={2018},
      eprint={1803.02453},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{agarwal2019fair,
      title={Fair Regression: Quantitative Definitions and Reduction-based Algorithms}, 
      author={Alekh Agarwal and Miroslav Dudík and Zhiwei Steven Wu},
      year={2019},
      eprint={1905.12843},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{calmon2017optimized,
      title={Optimized Data Pre-Processing for Discrimination Prevention}, 
      author={Flavio P. Calmon and Dennis Wei and Karthikeyan Natesan Ramamurthy and Kush R. Varshney},
      year={2017},
      eprint={1704.03354},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{wang2023mitigating,
      title={Mitigating Algorithmic Bias with Limited Annotations}, 
      author={Guanchu Wang and Mengnan Du and Ninghao Liu and Na Zou and Xia Hu},
      year={2023},
      eprint={2207.10018},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{esmaeilzadeh2022abuse,
      title={Abuse and Fraud Detection in Streaming Services Using Heuristic-Aware Machine Learning}, 
      author={Soheil Esmaeilzadeh and Negin Salajegheh and Amir Ziai and Jeff Boote},
      year={2022},
      eprint={2203.02124},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bhattacharya2022augmenting,
      title={Augmenting Netflix Search with In-Session Adapted Recommendations}, 
      author={Moumita Bhattacharya and Sudarshan Lamkhede},
      year={2022},
      eprint={2206.02254},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{roy2023machine,
      title={Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions}, 
      author={Mrinmoy Roy and Sarwar J. Minar and Porarthi Dhar and A T M Omor Faruq},
      year={2023},
      eprint={2307.14067},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{sen2021machine,
      title={Machine Learning in Finance-Emerging Trends and Challenges}, 
      author={Jaydip Sen and Rajdeep Sen and Abhishek Dutta},
      year={2021},
      eprint={2110.11999},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST}
}

@misc{fairmodels,
      title={fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation}, 
      author={Jakub Wiśniewski and Przemysław Biecek},
      year={2022},
      eprint={2104.00507},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{dis_impact_rem,
author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
title = {Certifying and Removing Disparate Impact},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783311},
doi = {10.1145/2783258.2783311},
abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses.We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {259–268},
numpages = {10},
keywords = {machine learning, fairness, disparate impact},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@misc{soumah2023radar,
      title={Radar de Parit\'e: An NLP system to measure gender representation in French news stories}, 
      author={Valentin-Gabriel Soumah and Prashanth Rao and Philipp Eibl and Maite Taboada},
      year={2023},
      eprint={2304.09982},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dinan2020multidimensional,
      title={Multi-Dimensional Gender Bias Classification}, 
      author={Emily Dinan and Angela Fan and Ledell Wu and Jason Weston and Douwe Kiela and Adina Williams},
      year={2020},
      eprint={2005.00614},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{examples_dis,
author = {Terence Shin},
title = {Real-Life Examples of Discriminating Artificial Intelligence},
year = {2020},
url = {https://towardsdatascience.com/real-life-examples-of-discriminating-artificial-intelligence-cae395a90070},
note = {Accedido: 1-12-2023}
}

@InProceedings{10.1007/978-3-031-35320-8_39,
author="Hirota, Haruka
and Kertkeidkachorn, Natthawut
and Shirai, Kiyoaki",
editor="M{\'e}tais, Elisabeth
and Meziane, Farid
and Sugumaran, Vijayan
and Manning, Warren
and Reiff-Marganiec, Stephan",
title="Weakly-Supervised Multimodal Learning for Predicting the Gender of Twitter Users",
booktitle="Natural Language Processing and Information Systems",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="522--532",
abstract="Social media platforms, e.g. Twitter, are significant sources of information, with various users posting vast amounts of content every day. Analyzing such content has the potential to offer valuable insights for commercial and research purposes. To gain a comprehensive understanding of the information, it is crucial to consider the demographics of users, with gender being a particularly important factor. Nevertheless, the gender of Twitter's users is not usually available. Predicting the gender of Twitter's users from tweet data becomes more challenging. In this paper, we introduce a weakly supervised method to automatically build the supervision data. The experimental result show that our weak supervision component could generate well-annotated data automatically with an accuracy rate exceeding 85{\%}. Furthermore, we conduct a comparative analysis of various multimodal learning architectures to predict the gender of Twitter users using weak supervision data. In the study, five multimodal learning architectures: 1) Early Fusion, 2) Late Fusion, 3) Dense Fusion, 4) Caption Fusion, and 5) Ensemble Fusion, are proposed. The experimental results on the evaluation data indicate that Caption Fusion outperforms the other multimodal learning architectures and baselines.",
isbn="978-3-031-35320-8"
}

@inproceedings{karkkainenfairface,
      title={FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation},
      author={Karkkainen, Kimmo and Joo, Jungseock},
      booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
      year={2021},
      pages={1548--1558}
    }

@Inproceedings{Currey2022,
 author = {Anna Currey and Maria Nădejde and Raghavendra Pappagari and Mia Mayer and Stanislas LAULY and Xing Niu and Benjamin Hsu and Georgiana Dinu},
 title = {MT-GenEval: A counterfactual and contextual dataset for evaluating gender accuracy in machine translation},
 year = {2022},
 url = {https://www.amazon.science/publications/mt-geneval-a-counterfactual-and-contextual-dataset-for-evaluating-gender-accuracy-in-machine-translation},
 booktitle = {EMNLP 2022},
}

@online{propublica,
author = {Julia Angwin and Jeff Larson and Surya Mattu and Lauren Kirchner},
title = {Machine Bias},
year = {2016},
url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
note = {Accedido: 20-11-2023}
}

@online{beauty,
author = {Sam Levin},
title = {A beauty contest was judged by AI and the robots didn't like dark skin},
year = {2016},
url = {https://www.theguardian.com/technology/2016/sep/08/artificial-intelligence-beauty-contest-doesnt-like-black-people},
note = {Accedido: 18-11-2023}
}