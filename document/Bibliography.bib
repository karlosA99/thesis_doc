@INPROCEEDINGS{autonomous_driving,
  author={Okuyama, Takafumi and Gonsalves, Tad and Upadhay, Jaychand},
  booktitle={2018 International Conference on Intelligent Autonomous Systems (ICoIAS)}, 
  title={Autonomous Driving System based on Deep Q Learnig}, 
  year={2018},
  volume={},
  number={},
  pages={201-205},
  doi={10.1109/ICoIAS.2018.8494053}}

@INPROCEEDINGS{fairness_def,
  author={Verma, Sahil and Rubin, Julia},
  booktitle={2018 IEEE/ACM International Workshop on Software Fairness (FairWare)}, 
  title={Fairness Definitions Explained}, 
  year={2018},
  volume={},
  number={},
  pages={1-7},
  doi={10.1145/3194770.3194776}}

@inproceedings{compas,
  title     = {mdfa: Multi-Differential Fairness Auditor for Black Box Classifiers},
  author    = {Gitiaux, Xavier and Rangwala, Huzefa},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {5871--5879},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/814},
  url       = {https://doi.org/10.24963/ijcai.2019/814},
}

@inproceedings{counterfactual,
 author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Fairness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{fair_awareness,
      title={Fairness Through Awareness}, 
      author={Cynthia Dwork and Moritz Hardt and Toniann Pitassi and Omer Reingold and Rich Zemel},
      year={2011},
      eprint={1104.3913},
      archivePrefix={arXiv},
      primaryClass={cs.CC}
}

@misc{subgroup_fairness,
      title={An Empirical Study of Rich Subgroup Fairness for Machine Learning}, 
      author={Michael Kearns and Seth Neel and Aaron Roth and Zhiwei Steven Wu},
      year={2018},
      eprint={1808.08166},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{resp_data,
author = {Stoyanovich, Julia and Howe, Bill and Jagadish, H. V.},
title = {Responsible Data Management},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415570},
doi = {10.14778/3415478.3415570},
abstract = {The need for responsible data management intensifies with the growing impact of data on society. One central locus of the societal impact of data are Automated Decision Systems (ADS), socio-legal-technical systems that are used broadly in industry, non-profits, and government. ADS process data about people, help make decisions that are consequential to people's lives, are designed with the stated goals of improving efficiency and promoting equitable access to opportunity, involve a combination of human and automated decision making, and are subject to auditing for legal compliance and to public disclosure. They may or may not use AI, and may or may not operate with a high degree of autonomy, but they rely heavily on data.In this article, we argue that the data management community is uniquely positioned to lead the responsible design, development, use, and oversight of ADS. We outline a technical research agenda that requires that we step outside our comfort zone of engineering for efficiency and accuracy, to also incorporate reasoning about values and beliefs. This seems high-risk, but one of the upsides is being able to explain to our children what we do and why it matters.},
journal = {Proc. VLDB Endow.},
month = {8},
pages = {3474--3488},
numpages = {15}
}

@misc{survey,
      title={A Survey on Bias and Fairness in Machine Learning}, 
      author={Ninareh Mehrabi and Fred Morstatter and Nripsuta Saxena and Kristina Lerman and Aram Galstyan},
      year={2022},
      eprint={1908.09635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{compas2,
author = {Julia Dressel  and Hany Farid },
title = {The accuracy, fairness, and limits of predicting recidivism},
journal = {Science Advances},
volume = {4},
number = {1},
pages = {eaao5580},
year = {2018},
doi = {10.1126/sciadv.aao5580},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.aao5580},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.aao5580},
abstract = {Should we trust computers to make life-altering decisions in the criminal justice system? Algorithms for predicting recidivism are commonly used to assess a criminal defendant’s likelihood of committing a crime. These predictions are used in pretrial, parole, and sentencing decisions. Proponents of these systems argue that big data and advanced machine learning make these analyses more accurate and less biased than humans. We show, however, that the widely used commercial risk assessment software COMPAS is no more accurate or fair than predictions made by people with little or no criminal justice expertise. In addition, despite COMPAS’s collection of 137 features, the same accuracy can be achieved with a simple linear classifier with only two features.}}

@misc{understanding,
      title={Understanding racial bias in health using the Medical Expenditure Panel Survey data}, 
      author={Moninder Singh and Karthikeyan Natesan Ramamurthy},
      year={2019},
      eprint={1911.01509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{adult,
      title={Identifying and examining machine learning biases on Adult dataset}, 
      author={Sahil Girhepuje},
      year={2023},
      eprint={2310.09373},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{datta2015automated,
      title={Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice, and Discrimination}, 
      author={Amit Datta and Michael Carl Tschantz and Anupam Datta},
      year={2015},
      eprint={1408.6491},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{sweeney2013discrimination,
      title={Discrimination in Online Ad Delivery}, 
      author={Latanya Sweeney},
      year={2013},
      eprint={1301.6822},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{unequal_rep,
author = {Kay, Matthew and Matuszek, Cynthia and Munson, Sean A.},
title = {Unequal Representation and Gender Stereotypes in Image Search Results for Occupations},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702520},
doi = {10.1145/2702123.2702520},
abstract = {Information environments have the power to affect people's perceptions and behaviors. In this paper, we present the results of studies in which we characterize the gender bias present in image search results for a variety of occupations. We experimentally evaluate the effects of bias in image search results on the images people choose to represent those careers and on people's perceptions of the prevalence of men and women in each occupation. We find evidence for both stereotype exaggeration and systematic underrepresentation of women in search results. We also find that people rate search results higher when they are consistent with stereotypes for a career, and shifting the representation of gender in image search results can shift people's perceptions about real-world distributions. We also discuss tensions between desires for high-quality results and broader societal goals for equality of representation in this space.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {3819–3828},
numpages = {10},
keywords = {inequality, gender, representation, image search, bias, stereotypes},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@techreport{facial_bias,
  doi = {10.5281/ZENODO.4050457},
  url = {https://zenodo.org/record/4050457},
  author = {Leslie, David},
  keywords = {facial recognition technologies, algorithmic bias, digital ethics, responsible innovation, biometric technologies},
  title = {Understanding bias in facial recognition technologies},
  publisher = {Zenodo},
  year = {2020}, 
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{chatbot_bias,
      title={Bias and Fairness in Chatbots: An Overview}, 
      author={Jintang Xue and Yun-Cheng Wang and Chengwei Wei and Xiaofeng Liu and Jonghye Woo and C. -C. Jay Kuo},
      year={2023},
      eprint={2309.08836},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{voice_bias,
author = {Bajorek, Joan},
year = {2019},
month = {05},
pages = {},
title = {Voice Recognition Still Has Significant Race and Gender Biases},
journal = {Harvard business review}
}

@article{preproc,
author = {Kamiran, Faisal and Calders, Toon},
year = {2011},
month = {10},
pages = {},
title = {Data Pre-Processing Techniques for Classification without Discrimination},
volume = {33},
journal = {Knowledge and Information Systems},
doi = {10.1007/s10115-011-0463-8}
}
@article{osti_10182459,
place = {Country unknown/Code not available}, 
title = {Fairness-Aware Instrumentation of Preprocessing~Pipelines for Machine Learning}, 
url = {https://par.nsf.gov/biblio/10182459}, 
DOI = {10.1145/3398730.3399194}, 
abstractNote = {Surfacing and mitigating bias in ML pipelines is a complex topic, with a dire need to provide system-level support to data scientists. Humans should be empowered to debug these pipelines, in order to control for bias and to improve data quality and representativeness. We propose fairDAGs, an open-source library that extracts directed acyclic graph (DAG) representations of the data flow in preprocessing pipelines for ML. The library subsequently instruments the pipelines with tracing and visualization code to capture changes in data distributions and identify distortions with respect to protected group membership as the data travels through the pipeline. We illustrate the utility of fairDAGs, with experiments on publicly available ML pipelines.}, 
journal = {Workshop on Human-In-the-Loop Data Analytics (HILDA'20)}, 
author = {Yang, Ke and Huang, Biao and Stoyanovich, Julia and Schelter, Sebastian}, }

@misc{donini2020empirical,
      title={Empirical Risk Minimization under Fairness Constraints}, 
      author={Michele Donini and Luca Oneto and Shai Ben-David and John Shawe-Taylor and Massimiliano Pontil},
      year={2020},
      eprint={1802.08626},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{zafar17a,
  title = 	 {{Fairness Constraints: Mechanisms for Fair Classification}},
  author = 	 {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P.},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {962--970},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {04},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/zafar17a.html},
  abstract = 	 {Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy.}
}

@inproceedings{ml_in_admissions,
author = {Martinez Neda, Barbara and Zeng, Yue and Gago-Masague, Sergio},
title = {Using Machine Learning in Admissions: Reducing Human and Algorithmic Bias in the Selection Process},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3439664},
doi = {10.1145/3408877.3439664},
abstract = {Diverse classrooms are linked to enhanced intellectual engagement and understanding of different perspectives. College admissions decisions have traditionally relied heavily on academic characteristics like GPA and standardized testing. Universities started to adopt holistic strategies while attempting to increase diversity. Yet, increasing subjective assessment may increase risk of human bias. Machine Learning (ML) could assist in admitting a more diverse student body, but algorithmic bias could be introduced. Our goal is to develop software tools to minimize human bias in admissions while actively eliminating algorithmic bias. We will examine past admissions data and identify risks of use of possible privileged values, which have historically put certain groups at a disadvantage. This tool may help reduce bias risk and select a more diverse and academically prepared group for admission.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {1323},
numpages = {1},
keywords = {diversity, college admission, machine learning},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@article{d_Alessandro_2017,
   title={Conscientious Classification: A Data Scientist’s Guide to Discrimination-Aware Classification},
   volume={5},
   ISSN={2167-647X},
   url={http://dx.doi.org/10.1089/big.2016.0048},
   DOI={10.1089/big.2016.0048},
   number={2},
   journal={Big Data},
   publisher={Mary Ann Liebert Inc},
   author={d’Alessandro, Brian and O’Neil, Cathy and LaGatta, Tom},
   year={2017},
   month=jun, pages={120–134} }

@article{seymour2018bias,
  title = "Detecting Bias: Does an Algorithm Have to Be Transparent in Order to Be Fair?",
  author = "William Seymour",
  year = "2018",
  journal = "BIAS 2018",
  url = "http://ceur-ws.org/Vol-2103/",
}

@misc{gitiaux2019multidifferential,
      title={Multi-Differential Fairness Auditor for Black Box Classifiers}, 
      author={Xavier Gitiaux and Huzefa Rangwala},
      year={2019},
      eprint={1903.07609},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{agarwal2018reductions,
      title={A Reductions Approach to Fair Classification}, 
      author={Alekh Agarwal and Alina Beygelzimer and Miroslav Dudík and John Langford and Hanna Wallach},
      year={2018},
      eprint={1803.02453},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{agarwal2019fair,
      title={Fair Regression: Quantitative Definitions and Reduction-based Algorithms}, 
      author={Alekh Agarwal and Miroslav Dudík and Zhiwei Steven Wu},
      year={2019},
      eprint={1905.12843},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{calmon2017optimized,
      title={Optimized Data Pre-Processing for Discrimination Prevention}, 
      author={Flavio P. Calmon and Dennis Wei and Karthikeyan Natesan Ramamurthy and Kush R. Varshney},
      year={2017},
      eprint={1704.03354},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{wang2023mitigating,
      title={Mitigating Algorithmic Bias with Limited Annotations}, 
      author={Guanchu Wang and Mengnan Du and Ninghao Liu and Na Zou and Xia Hu},
      year={2023},
      eprint={2207.10018},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{esmaeilzadeh2022abuse,
      title={Abuse and Fraud Detection in Streaming Services Using Heuristic-Aware Machine Learning}, 
      author={Soheil Esmaeilzadeh and Negin Salajegheh and Amir Ziai and Jeff Boote},
      year={2022},
      eprint={2203.02124},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bhattacharya2022augmenting,
      title={Augmenting Netflix Search with In-Session Adapted Recommendations}, 
      author={Moumita Bhattacharya and Sudarshan Lamkhede},
      year={2022},
      eprint={2206.02254},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}