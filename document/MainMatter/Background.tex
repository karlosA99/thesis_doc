\chapter{Justicia y equidad en modelos de aprendizaje autom\'atico}\label{chapter:state-of-the-art}

- Justicia / Sesgos / Equidad ok
- Casos Controversiales. ok
- Fuentes de Sesgos ok
- (Definiciones / Algoritmos) para tratar los sesgos ok
- Datasets Recursos de sesgos
- Discusi'on

Los algoritmos de aprendizaje autom\'atico han permeado pr\'acticamente todos los aspectos de la vida moderna,
desde proporcionar recomendaciones de pel\'iculas y facilitar compras en l\'inea hasta influir en b\'usquedas en la web
y sugerir conexiones emocionales con otras personas. Este fen\'omeno se extiende incluso a escenarios m\'as riesgosos,
como el diagn\'ostico y tratamiento m\'edico, donde el uso de estos algoritmos ha experimentado un notable aumento.
La versatilidad de estas herramientas abarca diversas \'areas, como la optimizaci\'on de procesos empresariales, la 
mejora de la eficiencia de los sistemas de transporte \cite{autonomous_driving}, y la personalizaci\'on de servicios en 
sectores como el financiero y el comercio.

A medida que estos algoritmos se aplican con mayor frecuencia en \'ambitos m\'as cr\'iticos, como
pr\'estamos bancarios \cite{fairness_def}, evaluaci\'on de riesgos de salud, contrataci\'on, evaluaci\'on de desempe\~no laboral y
justicia penal \cite{compas}, se genera una creciente preocupaci\'on acerca de su capacidad para mantener de manera involuntaria 
sesgos sociales y prejuicios hist\'oricos. 

\section{Sistemas sesgados}

    La presencia de sesgos en sistemas que utilizan modelos de aprendizaje autom\'atico es un tema cr\'itico que ha sido
    ampliamente estudiado en los \'ultimos a\~nos. En el \'ambito de la justicia penal, el caso m\'as conocido es el de COMPAS
    \footnote{\url{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}}
    (en ingl\'es \textit{Correctional Offender Management Profiling for Alternative Sanctions}). Este sistema es un algoritmo de predicci\'on
    de riesgo de reincidencia criminal que se ha utilizado en las cortes de Estados Unidos para ayudar a los jueces a determinar la 
    probabilidad de que una persona reincurra en un delito. Se demostr\'o que el software estaba sesgado hacia las personas 
    afroamericanas, o sea, dados dos individuos con el mismo perfil criminal, solo basta que uno sea afroamericano para que el
    sistema prediga que tiene una mayor probabilidad de reincidir respecto al que no lo es. En un estudio, se determin\'o que, 
    en comparaci\'on con la evaluaci\'on realizada por personas no expertas, el desempe\~no del sistema no demostr\'o mejoras 
    significativas \cite{compas2}. 

    Los datos de la Encuesta de Gastos M\'edicos (\texttt{MEPS}\footnote{\url{https://meps.ahrq.gov/mepsweb/about_meps/spanish.jsp}}, 
    por sus siglas en ingl\'es) son una colecci\'on de encuestas representativas a nivel nacional, de acceso p\'ublico, que proporcionan 
    datos sobre el uso y los costos de los servicios de atenci\'on m\'edica para la poblaci\'on civil no institucionalizada de los Estados Unidos. 
    Es com\'un el empleo de estos datos para el desarrollo de modelos predictivos de gastos de salud con el objetivo de guiar decisiones 
    en la gesti\'on de la atenci\'on m\'edica, enfermedades y costos asociados. Se ha comprobado que estos modelos tambi\'en capturan sesgos, 
    generando uun sesgo significativo en perjuicio de las personas afroamericanas. Concretamentea, existe una menor probabilidad de que 
    las personas afroamericanas sean identificadas como pacientes con altos gastos futuros en comparaci\'on con las personas personas blancas, 
    lo que resulta en una menor probabilidad de recibir gesti\'on de atenci\'on \cite{understanding}.

    Los sesgos y prejuicios presentes en la sociedad tambi\'en se han identificado en anuncios generados por modelos de aprendizaje autom\'atico
    \cite{sweeney2013discrimination,datta2015automated} y en motores de b\'usqueda web \cite{unequal_rep}, pricipalmente en relaci\'on al 
    g\'enero. Adem\'as, se han identificado sesgos en otros sistemas como los chatbots \cite{chatbot_bias}, as\'i como en sistemas de 
    reconocimiento facial \cite{facial_bias} y de voz \cite{voice_bias}, y algunos sistemas empleados en concursos de belleza(poner esto al pie
    https://www.theguardian.com/technology/2016/sep/08/artificial-intelligence-beauty-contest-doesnt-like-black-people).

\section{Fuentes y tipos de sesgos}

    Las principales fuentes de sesgo incluyen caracter\'isticas en los datos, decisiones de dise\~no algor\'itmico y las interacciones 
    con los usuarios \cite{resp_data}.
    
    No existe una desici\'on un\'anime en cuanto a la clasificaci\'on de sesgos seg\'un su tipo, sin embargo, 
    algunos siempre son contemplados y se reconocen como los m\'as relevantes \cite{survey}. La figura $PONER FIGURA DEL CICLO$ muestra 
    el ciclo de vida de los datos en un sistema de aprendizaje autom\'atico; siguiendo este enfoque los sesgos pueden clasificarse 
    en funci\'on de la fase espec\'ifica en que puedan surgir dentro de dicho ciclo: \textbf{sesgos de los datos al algoritmo}, 
    \textbf{sesgos del algoritmo al usuario} y \textbf{sesgos del usuario a los datos}.
    
    \subsection{Sesgos de los datos al algoritmo}
    
    \begin{itemize}
        \item \textbf{Sesgo por medici\'on}: Este sesgo surge de como se eligen, utilizan y miden atributos particulares. Un ejemplo de esto se observa 
        en la herramienta de predicci\'on de riesgo de reincidencia COMPAS, donde detenciones anteriores y detenciones de amigos o familiares se utilizaron 
        como variables para medir peligrosidad.
        
        \item \textbf{Sesgo por variable omitida}: Este sesgo ocurre cuando una o m\'as variables importantes son excluidas del modelo. Decir algo mas
        
        \item \textbf{Sesgo por representaci\'on}: Este sesgo se produce de c\'omo se seleccionan las muestras de una poblaci\'on durante el proceso de
        recopilaci\'on de datos. Las muestras no representativas carecen de la diversidad de la poblaci\'on, con subgrupos faltantes y otras anomal\'ias.
    \end{itemize}
    
    \subsection{Sesgos del algoritmo al usuario}

    \begin{itemize}
        \item \textbf{Sesgo por algor\'itmico}: El sesgo algor\'itmico se presenta cuando el sesgo no est\'a presente en los datos de entrada
        y se agrega puramente por el algoritmo. Las elecciones de dise\~no algor\'itmico, como el uso de ciertas funciones de optimizaci\'on, 
        regularizaciones, decisiones en la aplicaci\'on de modelos de regresi\'on en los datos en su totalidad o considerando subgrupos, y el uso
        general de estimadores estad\'isticamente sesgados en algoritmos, todos pueden contribuir a decisiones algor\'itmicas sesgadas que 
        afectan los resultados de los algoritmos. 
        
        \item \textbf{Sesgo por la interacci\'on del usuario}: El sesgo de interacci\'on del usuario no solo puede observarse en la web, sino que 
        tambi\'en puede ser producido por dos fuentes: la interfaz de usuario y cuando el propio usuario impone su comportamiento sesgado.
        
        \item \textbf{Sesgo de evaluaci\'on}: El sesgo de evaluci\'on ocurre durante la evaluaci\'on del modelo. Esto incluye el uso de m\'etricas
        inapropiadas y desproporcionadas para la evaluaci\'on del modelo. Un ejemplo de esto son las m\'etricas \textit{Adience} y \textit{IJB-A}, 
        que se utilizan en la evaluaci\'on de sistemas de reconocimiento facial que estaban sesgados hacia el color de la piel y el g\'enero.
    \end{itemize}
    
    \subsection{Sesgos del usuario a los datos}

    \begin{itemize}
        \item \textbf{Sesgo hist\'orico}: El sesgo hist\'orico es el sesgo y los problemas sociot\'ecnicos ya existentes en el mundo y puede 
        generarse desde el proceso de generaci\'on de datos incluso con un muestreo y selecci\'on de caracter\'isticas perfectos.

        \item \textbf{Sesgo poblacional}: El sesgo poblacional surge cuando las estad\'isticas, demograf\'ias, representantes y caracter\'isticas 
        de los usuarios son diferentes en la poblaci\'on de usuarios de la plataforma con respecto a la poblaci\'on objetivo original, creando datos no
        representativos. Un ejemplo de este tipo de sesgo puede surgir de las diferentes demograf\'ias de usuarios en las plataformas sociales, como las
        mujeres que son m\'as propensas a usar \textit{Pinterest}, \textit{Facebook}, \textit{Instagram}, mientras que los hombres son m\'as activos en 
        foros en l\'inea como \textit{Reddit} o \textit{X}.

        \item \textbf{Sesgo social}: El sesgo social se produce cuando las acciones de otros afectan el juicio de una persona. Un ejemplo de este tipo de 
        sesgo podr\'ia ser un caso en el que la persona quiere calificar o revisar un elemento con puntuaci\'on baja, pero al ser influenciada por otras
        calificaciones altas, cambia su puntuaci\'on a una calificaci\'on m\'as alta, pensando que quiz\'as esta siendo demasiado severa.
    \end{itemize}

\section{Detecci\'on y mitigaci\'on de sesgos}

Las definiciones de equidad en el contexto de los modelos de aprendizaje de m\'aquinas nos llevan 
a un panorama complejo y en constante evoluci\'on. A d\'ia de hoy, no existe una definici\'on \'unica y precisa
de lo que constituye la equidad en este \'ambito. La implementaci\'on de algoritmos en la toma de decisiones automatizada ha desatado 
debates acerca de c\'omo conceptualizar y medir tanto la equidad como la justicia. Estos conceptos no solo involucran consideraciones 
t\'ecnicas, sino que tambi\'en se ven influidos por matices culturales y dilemas \'eticos.

\subsection{Definiciones de equidad}
Las diversas perspectivas sobre la equidad pueden agruparse en dos categor\'ias principales: a nivel de Grupos y a nivel Individual. 
A continuaci\'on se presentan algunas de las definiciones de equidad m\'as relevantes a nivel de grupos:

\begin{itemize}
    \item \textbf{Demographic Parity}: Un algoritmo predictor $\hat{Y}$ satisface \textit{Demographic Parity}
    con respecto a un atributo $A$ con valores en el conjunto $\{0,1\}$ si se cumple $P(\hat{Y} | A = 0) = P(\hat{Y} | A = 1)$. 
    Esto significa que la probabilidad de un resultado positivo debería ser la misma sin importar si el individuo pertenece a
    un grupo protegido \cite{fairness_def}.

    \item \textbf{Equal Opportunity}: Un predictor binario $\hat{Y}$ satisface \textit{Equal Opportunity} con 
    respecto a un atributo $A$ y salida $Y$ si $P(\hat{Y} = 1 | A = 1, Y = 1) = P(\hat{Y} = 1 | A = 0, Y = 1)$. Esto significa
    que la probabilidad de que a una persona en la clase positiva le sea asignada un resultado positivo 
    deber\'ia ser igual para miembros tanto de grupos protegidos como no protegidos \cite{fairness_def}.

    \item \textbf{Equalized Odds}: Un predictor $\hat{Y}$ satisface \textit{Equalized Odds} con respecto a un atributo
    protegido $A$ y predicci\'on $Y$, si $P(\hat{Y} = 1 | A = 1, Y = y) = P(\hat{Y} = 1 | A = 0, Y = y)$, es decir,
    $\hat{Y}$ y $A$ son independientemente condicionales a $Y$. Esto significa que la probabilidad de que a una persona 
    en la clase positiva le sea asignada correctamente una predicci\'on positiva y la probabilidad de que a una persona en la 
    clase negativa le sea incorrectamente asignada una predicci\'on positiva deber\'ia se la misma para miembros de grupos 
    protegidos y no protegidos \cite{fairness_def}.
\end{itemize}

Otros enfoques relevantes para tratar los sesgos a nivel individual son:

\begin{itemize}
    \item \textbf{Fairness Through Awareness}: Individuos con caracter\'isticas similares seg\'un alg\'un criterio 
    definido deber\'ian obtener resultados parecidos \cite{fair_awareness}.
    \item \textbf{Fairness Through Unawareness}: Un algoritmo se considera imparcial cuando no basa sus decisiones en el 
    atributo protegido \cite{counterfactual}.
    \item \textbf{Conterfactual fairness}: Una decisi\'on es considerada imparcial hacia un individuo cuando es la misma
    tanto en la situaci\'on real como en una situaci\'on hipot\'etica donde el individuo pertenece a otro grupo \cite{counterfactual}.
\end{itemize}

\subsection{Mitigaci\'on de sesgos}

Los m\'etodos para la mitigaci\'on de sesgos se pueden clasificar esencialmente en m\'etodos de pre-procesamiento \cite{osti_10182459}, 
durante el procesamiento \cite{ml_in_admissions} y post-procesamiento \cite{gitiaux2019multidifferential}, seg\'un la fase del proceso de 
aprendizaje en que se realizen. Recientemente, han surgido nuevos m\'etodos llamados meta-algoritmos que ofrecen resultados interesantes 
en diversos escenarios.

Las estrategias de pre-procesamiento buscan la equidad al modificar la representaci\'on de los datos antes de aplicar un modelo de aprendizaje
autom\'atico. En este proceso se pueden aplicar diversas t\'ecnicas sobre los datos, como eliminar atributos protegidos y atributos correlacionados
con estos, o bien, modificar las etiquetas de algunos objetos en el datset \cite{preproc}. Una ventaja de estas t\'ecnicas es que son independientes
al modelo. Sin embargo, requieren ajustes de hiperpar\'ametros tanto propios como del modelo seleccionado para optimizar su desempe\~no.

Los m\'etodos aplicados durante el procesamiento, modifican los algoritmos de aprendizaje para eliminar la fuente de discriminaci\'on. Esto se 
logra mediante ajustes en la funci\'on objetivo o la aplicaci\'on de restricciones espec\'ificas \cite{donini2020empirical,zafar17a}. A pesar de 
que estas t\'ecnicas pueden ser altamente efectivas para la clase espec\'ifica de modelos para la cual fueron dise\~nadas, resulta dif\'icil, e 
incluso en ocasiones imposible, extenderlas a nuevas clases de modelos.

Las t\'ecnicas de post-procesamiento se implementan despu\'es de que el modelo ha sido entrenado, utilizando un conjunto de datos que no haya
participado en dicho proceso. Mediante este procesamiento, las clasificaciones generadas por el modelo se reasignan mediante una funci\'on 
espec\'ifica \cite{d_Alessandro_2017}.
Entre las t\'ecnicas de post-procesamiento, se incluyen aquellas que buscan identificar los atributos protegidos que afectan el resultado del 
modelo, y a partir de esto, ajustan la predicci\'on \cite{seymour2018bias}. La principal limitaci\'on radica en que ajustar la predicci\'on 
en esta fase es inherentemente sub\'optimo y puede resultar en un peor equilibrio entre eficacia y equidad.

Por \'ultimo, cabe destacar la relevancia de los meta-algoritmos, una categor\'ia de m\'etodos recientemente propuesta para problemas de 
mitigaci\'on de sesgos. Estos simplifican la tarea de mitigaci\'on de sesgos a una serie de problemas de clasificaci\'on, cada uno con 
un costo asociado a sus errores de predicci\'on \cite{agarwal2018reductions, agarwal2019fair}.
A diferencia de4 los m\'etodos operan durante el procesamiento, los meta-algoritmos son independientes al tipo de modelo utilizado en el 
clasificador base, solo dependen de la capacidad de este para ser reentrenado repetidamente. Las soluciones a estos problemas suelen generar
un clasificador randomizado.

\section{Hablar de los datasets estudiados}
    \begin{itemize}
        \item Poner la tabla comparativa de los datasets
        \item Decir algunos datos interesantes de cada uno
        \item ¿Empezar a hablar de xq se escogio imdb?
        \item hablar de que hace los datasets ayudan a mitigar y todo eso(ver donde ponerlo si al final del sub anterior o principio de este)
    \end{itemize}

\section{Discusi\'on}

