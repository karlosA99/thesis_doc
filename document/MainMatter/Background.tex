\chapter{Justicia y equidad en modelos de aprendizaje autom\'atico}\label{chapter:state-of-the-art}

- Justicia / Sesgos / Equidad
- Casos Controversiales.
- Fuentes de Sesgos
- (Definiciones / Algoritmos) para tratar los sesgos
- Datasets Recursos de sesgos
- Discusi'on

Los algoritmos de aprendizaje autom\'atico han permeado pr\'acticamente todos los aspectos de la vida moderna,
desde proporcionar recomendaciones de pel\'iculas y facilitar compras en l\'inea hasta influir en b\'usquedas en la web
y sugerir conexiones emocionales con otras personas. Este fen\'omeno se extiende incluso a escenarios m\'as riesgosos,
como el diagn\'ostico y tratamiento m\'edico, donde el uso de estos algoritmos ha experimentado un notable aumento.
La versatilidad de estas herramientas abarca diversas \'areas, como la optimizaci\'on de procesos empresariales, la 
mejora de la eficiencia de los sistemas de transporte \cite{autonomous_driving}, y la personalizaci\'on de servicios en 
sectores como el financiero y el comercio.

A medida que estos algoritmos se aplican con mayor frecuencia en \'ambitos m\'as cr\'iticos, como
pr\'estamos bancarios \cite{fairness_def}, evaluaci\'on de riesgos de salud, contrataci\'on, evaluaci\'on de desempe\~no laboral y
justicia penal \cite{compas}, se genera una creciente preocupaci\'on acerca de su capacidad para mantener de manera involuntaria 
sesgos sociales y prejuicios hist\'oricos. 

\section{Hablar de Casos reales donde se haya detectado sesgos}

    \begin{itemize}
        \item Hablar del caso COMPAS, MEPS y Adult. Ver los paper que se mencionan en el Survey1 en pag 8
    \end{itemize}

\section{Fuentes y tipos de sesgos}

    Las principales fuentes de sesgo incluyen caracter\'isticas en los datos, decisiones de dise\~no algor\'itmico y las interacciones 
    con los usuarios \cite{resp_data}.
    
    No existe una desici\'on un\'anime en cuanto a la clasificaci\'on de sesgos seg\'un su tipo, sin embargo, 
    algunos siempre son contemplados y se reconocen como los m\'as relevantes \cite{survey}. La figura $PONER FIGURA DEL CICLO$ muestra 
    el ciclo de vida de los datos en un sistema de aprendizaje autom\'atico; siguiendo este enfoque los sesgos pueden clasificarse 
    en funci\'on de la fase espec\'ifica en que puedan surgir dentro de dicho ciclo: \textbf{sesgos de los datos al algoritmo}, 
    \textbf{sesgos del algoritmo al usuario} y \textbf{sesgos del usuario a los datos}.
    
    \subsection{Sesgos de los datos al algoritmo}
    
    \begin{itemize}
        \item \textbf{Sesgo por medici\'on}: Este sesgo surge de como se eligen, utilizan y miden atributos particulares. Un ejemplo de esto se observa 
        en la herramienta de predicci\'on de riesgo de reincidencia COMPAS, donde detenciones anteriores y detenciones de amigos o familiares se utilizaron 
        como variables para medir peligrosidad.
        
        \item \textbf{Sesgo por variable omitida}: Este sesgo ocurre cuando una o m\'as variables importantes son excluidas del modelo. Decir algo mas
        
        \item \textbf{Sesgo por representaci\'on}: Este sesgo se produce de c\'omo se seleccionan las muestras de una poblaci\'on durante el proceso de
        recopilaci\'on de datos. Las muestras no representativas carecen de la diversidad de la poblaci\'on, con subgrupos faltantes y otras anomal\'ias.
    \end{itemize}
    
    \subsection{Sesgos del algoritmo al usuario}

    \begin{itemize}
        \item \textbf{Sesgo por algor\'itmico}: El sesgo algor\'itmico se presenta cuando el sesgo no est\'a presente en los datos de entrada
        y se agrega puramente por el algoritmo. Las elecciones de dise\~no algor\'itmico, como el uso de ciertas funciones de optimizaci\'on, 
        regularizaciones, decisiones en la aplicaci\'on de modelos de regresi\'on en los datos en su totalidad o considerando subgrupos, y el uso
        general de estimadores estad\'isticamente sesgados en algoritmos, todos pueden contribuir a decisiones algor\'itmicas sesgadas que 
        afectan los resultados de los algoritmos. 
        
        \item \textbf{Sesgo por la interacci\'on del usuario}: El sesgo de interacci\'on del usuario no solo puede observarse en la web, sino que 
        tambi\'en puede ser producido por dos fuentes: la interfaz de usuario y cuando el propio usuario impone su comportamiento sesgado.
        
        \item \textbf{Sesgo de evaluaci\'on}: El sesgo de evaluci\'on ocurre durante la evaluaci\'on del modelo. Esto incluye el uso de m\'etricas
        inapropiadas y desproporcionadas para la evaluaci\'on del modelo. Un ejemplo de esto son las m\'etricas \textit{Adience} y \textit{IJB-A}, 
        que se utilizan en la evaluaci\'on de sistemas de reconocimiento facial que estaban sesgados hacia el color de la piel y el g\'enero.
    \end{itemize}
    
    \subsection{Sesgos del usuario a los datos}

    \begin{itemize}
        \item \textbf{Sesgo hist\'orico}: El sesgo hist\'orico es el sesgo y los problemas sociot\'ecnicos ya existentes en el mundo y puede 
        generarse desde el proceso de generaci\'on de datos incluso con un muestreo y selecci\'on de caracter\'isticas perfectos.

        \item \textbf{Sesgo poblacional}: El sesgo poblacional surge cuando las estad\'isticas, demograf\'ias, representantes y caracter\'isticas 
        de los usuarios son diferentes en la poblaci\'on de usuarios de la plataforma con respecto a la poblaci\'on objetivo original, creando datos no
        representativos. Un ejemplo de este tipo de sesgo puede surgir de las diferentes demograf\'ias de usuarios en las plataformas sociales, como las
        mujeres que son m\'as propensas a usar \textit{Pinterest}, \textit{Facebook}, \textit{Instagram}, mientras que los hombres son m\'as activos en 
        foros en l\'inea como \textit{Reddit} o \textit{X}.

        \item \textbf{Sesgo social}: El sesgo social se produce cuando las acciones de otros afectan el juicio de una persona. Un ejemplo de este tipo de 
        sesgo podr\'ia ser un caso en el que la persona quiere calificar o revisar un elemento con puntuaci\'on baja, pero al ser influenciada por otras
        calificaciones altas, cambia su puntuaci\'on a una calificaci\'on m\'as alta, pensando que quiz\'as esta siendo demasiado severa.
    \end{itemize}

\section{Detecci\'on y mitigaci\'on de sesgos}

Las definiciones de equidad en el contexto de los modelos de aprendizaje de m\'aquinas nos llevan 
a un panorama complejo y en constante evoluci\'on. A d\'ia de hoy, no existe una definici\'on \'unica y precisa
de lo que constituye la equidad en este \'ambito. La implementaci\'on de algoritmos en la toma de decisiones automatizada ha desatado 
debates acerca de c\'omo conceptualizar y medir tanto la equidad como la justicia. Estos conceptos no solo involucran consideraciones 
t\'ecnicas, sino que tambi\'en se ven influidos por matices culturales y dilemas \'eticos.

\subsection{Definiciones de equidad}
Las diversas perspectivas sobre la equidad pueden agruparse en dos categor\'ias principales: a nivel de Grupos y a nivel Individual. 
A continuaci\'on se presentan algunas de las definiciones de equidad m\'as relevantes a nivel de grupos:

\begin{itemize}
    \item \textbf{Demographic Parity}: Un algoritmo predictor $\hat{Y}$ satisface \textit{Demographic Parity}
    con respecto a un atributo $A$ con valores en el conjunto $\{0,1\}$ si se cumple $P(\hat{Y} | A = 0) = P(\hat{Y} | A = 1)$. 
    Esto significa que la probabilidad de un resultado positivo debería ser la misma sin importar si el individuo pertenece a
    un grupo protegido \cite{fairness_def}.

    \item \textbf{Equal Opportunity}: Un predictor binario $\hat{Y}$ satisface \textit{Equal Opportunity} con 
    respecto a un atributo $A$ y salida $Y$ si $P(\hat{Y} = 1 | A = 1, Y = 1) = P(\hat{Y} = 1 | A = 0, Y = 1)$. Esto significa
    que la probabilidad de que a una persona en la clase positiva le sea asignada un resultado positivo 
    deber\'ia ser igual para miembros tanto de grupos protegidos como no protegidos \cite{fairness_def}.

    \item \textbf{Equalized Odds}: Un predictor $\hat{Y}$ satisface \textit{Equalized Odds} con respecto a un atributo
    protegido $A$ y predicci\'on $Y$, si $P(\hat{Y} = 1 | A = 1, Y = y) = P(\hat{Y} = 1 | A = 0, Y = y)$, es decir,
    $\hat{Y}$ y $A$ son independientemente condicionales a $Y$. Esto significa que la probabilidad de que a una persona 
    en la clase positiva le sea asignada correctamente una predicci\'on positiva y la probabilidad de que a una persona en la 
    clase negativa le sea incorrectamente asignada una predicci\'on positiva deber\'ia se la misma para miembros de grupos 
    protegidos y no protegidos \cite{fairness_def}.
\end{itemize}

Otros enfoques relevantes para tratar los sesgos a nivel individual son:

\begin{itemize}
    \item \textbf{Fairness Through Awareness}: Individuos que comparten caracter\'isticas similares seg\'un alg\'un criterio 
    definido deber\'ian obtener resultados parecidos \cite{fair_awareness}.
    \item \textbf{Fairness Through Unawareness}: Un algoritmo se considera imparcial cuando no basa sus decisiones en el 
    atributo protegido \cite{counterfactual}.
    \item \textbf{Conterfactual fairness}: Una decisi\'on es considerada imparcial hacia un individuo cuando es la misma
    tanto en la situaci\'on real como en una situaci\'on hipot\'etica donde el individuo pertenece a otro grupo \cite{counterfactual}.
\end{itemize}

\subsection{Algoritmos de mitigaci\'on de sesgos}

Hablar ahora aqui algo sencillo y muy general sobre los algos de mitigacion

\section{Hablar de los datasets estudiados}
    \begin{itemize}
        \item Poner la tabla comparativa de los datasets
        \item Decir algunos datos interesantes de cada uno
        \item ¿Empezar a hablar de xq se escogio imdb?
        \item hablar de que hace los datasets ayudan a mitigar y todo eso(ver donde ponerlo si al final del sub anterior o principio de este)
    \end{itemize}

\section{Discusi\'on}

